#!/usr/bin/env python3
"""
ä¸‰æ•°æ®åº“å‹åŠ›æµ‹è¯•å¯¹æ¯”ï¼šApexBase vs SQLite vs DuckDB

æµ‹è¯•åœºæ™¯ï¼š
1. ç¦»çº¿æ‰¹é‡å†™å…¥ - å¤§æ•°æ®é‡ETL
2. ç¦»çº¿å¤æ‚æŸ¥è¯¢ - åˆ†æå‹å·¥ä½œè´Ÿè½½  
3. åœ¨çº¿é«˜å¹¶å‘è¯»å†™ - æ··åˆå·¥ä½œè´Ÿè½½
4. åœ¨çº¿å»¶è¿Ÿæ•æ„Ÿ - å®æ—¶æŸ¥è¯¢

Usage:
    conda run -n dev python benchmarks/stress_test_comparison.py
"""

import os, sys, gc, time, tempfile, json, threading, random
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from typing import List, Dict, Tuple, Any
import sqlite3
import duckdb

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'apexbase', 'python'))

try:
    from apexbase import ApexClient
    HAS_APEX = True
except ImportError:
    HAS_APEX = False
    print("WARNING: ApexBase not found")

# â”€â”€ é…ç½® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@dataclass
class TestConfig:
    # ç¦»çº¿æµ‹è¯•é…ç½®
    offline_batch_size: int = 500_000      # 50ä¸‡è¡Œæ‰¹é‡å†™å…¥
    offline_query_iters: int = 10           # æŸ¥è¯¢è¿­ä»£æ¬¡æ•°
    
    # åœ¨çº¿æµ‹è¯•é…ç½®
    online_concurrent_threads: int = 10     # å¹¶å‘çº¿ç¨‹æ•°
    online_ops_per_thread: int = 200        # æ¯çº¿ç¨‹æ“ä½œæ•°
    
    # æ•°æ®é…ç½®
    n_cities: int = 10
    n_categories: int = 20

config = TestConfig()

def timer(func):
    def wrapper(*args, **kwargs):
        t0 = time.perf_counter()
        result = func(*args, **kwargs)
        elapsed = time.perf_counter() - t0
        return result, elapsed
    return wrapper

def generate_test_data(n_rows: int, seed: int = 42) -> List[Dict]:
    """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
    rng = np.random.default_rng(seed)
    
    cities = [f"City_{i}" for i in range(config.n_cities)]
    categories = [f"Category_{i}" for i in range(config.n_categories)]
    
    data = []
    for i in range(n_rows):
        record = {
            '_id': i + 1,
            'user_id': rng.integers(1, 50_000),
            'category': rng.choice(categories),
            'city': rng.choice(cities),
            'price': rng.uniform(10.0, 1000.0),
            'quantity': rng.integers(1, 100),
            'timestamp': int(time.time() - rng.integers(0, 86400 * 30)),
            'is_active': rng.choice([True, False], p=[0.8, 0.2]),
            'score': rng.uniform(0.0, 1.0)
        }
        data.append(record)
    
    return data

# â”€â”€ ApexBase æµ‹è¯• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class ApexBaseTester:
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.client = None
    
    def setup(self, data: List[Dict] = None):
        self.client = ApexClient(self.db_path)
        # å°è¯•ä½¿ç”¨è¡¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        try:
            self.client.use_table('sales_data')
        except:
            self.client.create_table('sales_data')
            # å¦‚æœæœ‰æ•°æ®ï¼Œå†™å…¥æ•°æ®
            if data is not None:
                self._write_data(data)
    
    def _write_data(self, data: List[Dict]):
        """å†™å…¥æ•°æ®"""
        batch_size = 25_000
        for i in range(0, len(data), batch_size):
            batch = data[i:i+batch_size]
            columns = {}
            for key in batch[0].keys():
                columns[key] = [record[key] for record in batch]
            self.client.store(columns)
    
    def cleanup(self):
        if self.client:
            self.client.close()
    
    @timer
    def test_batch_write(self, data: List[Dict]) -> Dict:
        """æ‰¹é‡å†™å…¥æµ‹è¯•"""
        print(f"    ApexBase æ‰¹é‡å†™å…¥ {len(data):,} æ¡è®°å½•...")
        
        batch_size = 25_000
        total_time = 0
        
        for i in range(0, len(data), batch_size):
            batch = data[i:i+batch_size]
            start_time = time.perf_counter()
            
            columns = {}
            for key in batch[0].keys():
                columns[key] = [record[key] for record in batch]
            
            self.client.store(columns)
            total_time += time.perf_counter() - start_time
        
        return {
            'rows_written': len(data),
            'total_time': total_time,
            'rows_per_sec': len(data) / total_time
        }
    
    @timer
    def test_queries(self) -> Dict:
        """æŸ¥è¯¢æµ‹è¯•"""
        queries = [
            "SELECT city, COUNT(*) as cnt, AVG(price) as avg_price FROM sales_data GROUP BY city ORDER BY cnt DESC",
            "SELECT category, COUNT(*) as total_count, AVG(price) as avg_price FROM sales_data GROUP BY category ORDER BY total_count DESC",
            "SELECT COUNT(*) as total_orders, SUM(price) as total_revenue FROM sales_data WHERE is_active = true",
            "SELECT city, AVG(score) as avg_score FROM sales_data WHERE price > 100 GROUP BY city ORDER BY avg_score DESC",
            "SELECT _id, city, price FROM sales_data WHERE price BETWEEN 50 AND 200 ORDER BY price DESC LIMIT 1000"
        ]
        
        query_times = []
        
        for i, query in enumerate(queries):
            times = []
            for _ in range(config.offline_query_iters):
                gc.collect()
                start = time.perf_counter()
                result = self.client.execute(query)
                elapsed = time.perf_counter() - start
                times.append(elapsed)
            
            query_times.extend(times)
        
        return {
            'total_queries': len(queries) * config.offline_query_iters,
            'avg_query_time': sum(query_times) / len(query_times),
            'queries_per_sec': 1.0 / (sum(query_times) / len(query_times))
        }

# â”€â”€ SQLite æµ‹è¯• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class SQLiteTester:
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.conn = None
    
    def setup(self):
        self.conn = sqlite3.connect(self.db_path)
        # å¦‚æœè¡¨ä¸å­˜åœ¨åˆ™åˆ›å»º
        try:
            self.conn.execute('''
                CREATE TABLE sales_data (
                    _id INTEGER PRIMARY KEY,
                    user_id INTEGER,
                    category TEXT,
                    city TEXT,
                    price REAL,
                    quantity INTEGER,
                    timestamp INTEGER,
                    is_active INTEGER,
                    score REAL
                )
            ''')
            self.conn.commit()
        except sqlite3.OperationalError:
            pass  # è¡¨å·²å­˜åœ¨
    
    def cleanup(self):
        if self.conn:
            self.conn.close()
    
    @timer
    def test_batch_write(self, data: List[Dict]) -> Dict:
        """æ‰¹é‡å†™å…¥æµ‹è¯•"""
        print(f"    SQLite æ‰¹é‡å†™å…¥ {len(data):,} æ¡è®°å½•...")
        
        # SQLite uses _id directly (same as ApexBase)
        start_time = time.perf_counter()
        
        # ä½¿ç”¨executemanyæ‰¹é‡æ’å…¥
        self.conn.executemany('''
            INSERT INTO sales_data VALUES 
            (:_id, :user_id, :category, :city, :price, :quantity, :timestamp, :is_active, :score)
        ''', data)
        self.conn.commit()
        
        total_time = time.perf_counter() - start_time
        
        return {
            'rows_written': len(data),
            'total_time': total_time,
            'rows_per_sec': len(data) / total_time
        }
    
    @timer
    def test_queries(self) -> Dict:
        """æŸ¥è¯¢æµ‹è¯•"""
        queries = [
            "SELECT city, COUNT(*) as cnt, AVG(price) as avg_price FROM sales_data GROUP BY city ORDER BY cnt DESC",
            "SELECT category, COUNT(*) as total_count, AVG(price) as avg_price FROM sales_data GROUP BY category ORDER BY total_count DESC",
            "SELECT COUNT(*) as total_orders, SUM(price) as total_revenue FROM sales_data WHERE is_active = 1",
            "SELECT city, AVG(score) as avg_score FROM sales_data WHERE price > 100 GROUP BY city ORDER BY avg_score DESC",
            "SELECT * FROM sales_data WHERE price BETWEEN 50 AND 200 ORDER BY price DESC LIMIT 1000"
        ]
        
        query_times = []
        
        for i, query in enumerate(queries):
            times = []
            for _ in range(config.offline_query_iters):
                gc.collect()
                start = time.perf_counter()
                self.conn.execute(query)
                elapsed = time.perf_counter() - start
                times.append(elapsed)
            
            query_times.extend(times)
        
        return {
            'total_queries': len(queries) * config.offline_query_iters,
            'avg_query_time': sum(query_times) / len(query_times),
            'queries_per_sec': 1.0 / (sum(query_times) / len(query_times))
        }

# â”€â”€ DuckDB æµ‹è¯• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class DuckDBTester:
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.conn = None
    
    def setup(self):
        self.conn = duckdb.connect(self.db_path)
        # å¦‚æœè¡¨ä¸å­˜åœ¨åˆ™åˆ›å»º
        try:
            # Use _id to match ApexBase
            self.conn.execute('''
                CREATE TABLE sales_data (
                    _id INTEGER,
                    user_id INTEGER,
                    category VARCHAR,
                    city VARCHAR,
                    price DOUBLE,
                    quantity INTEGER,
                    timestamp BIGINT,
                    is_active BOOLEAN,
                    score DOUBLE
                )
            ''')
        except Exception:
            pass  # è¡¨å·²å­˜åœ¨
    
    def cleanup(self):
        if self.conn:
            self.conn.close()
    
    @timer
    def test_batch_write(self, data: List[Dict]) -> Dict:
        """æ‰¹é‡å†™å…¥æµ‹è¯•"""
        print(f"    DuckDB æ‰¹é‡å†™å…¥ {len(data):,} æ¡è®°å½•...")
        
        start_time = time.perf_counter()
        
        # è½¬æ¢ä¸ºDataFrameç„¶åæ‰¹é‡æ’å…¥
        import pandas as pd
        df = pd.DataFrame(data)
        self.conn.execute('INSERT INTO sales_data SELECT * FROM df')
        
        total_time = time.perf_counter() - start_time
        
        return {
            'rows_written': len(data),
            'total_time': total_time,
            'rows_per_sec': len(data) / total_time
        }
    
    @timer
    def test_queries(self) -> Dict:
        """æŸ¥è¯¢æµ‹è¯•"""
        queries = [
            "SELECT city, COUNT(*) as cnt, AVG(price) as avg_price FROM sales_data GROUP BY city ORDER BY cnt DESC",
            "SELECT category, COUNT(*) as total_count, AVG(price) as avg_price FROM sales_data GROUP BY category ORDER BY total_count DESC",
            "SELECT COUNT(*) as total_orders, SUM(price) as total_revenue FROM sales_data WHERE is_active = true",
            "SELECT city, AVG(score) as avg_score FROM sales_data WHERE price > 100 GROUP BY city ORDER BY avg_score DESC",
            "SELECT * FROM sales_data WHERE price BETWEEN 50 AND 200 ORDER BY price DESC LIMIT 1000"
        ]
        
        query_times = []
        
        for i, query in enumerate(queries):
            times = []
            for _ in range(config.offline_query_iters):
                gc.collect()
                start = time.perf_counter()
                self.conn.execute(query).fetchall()
                elapsed = time.perf_counter() - start
                times.append(elapsed)
            
            query_times.extend(times)
        
        return {
            'total_queries': len(queries) * config.offline_query_iters,
            'avg_query_time': sum(query_times) / len(query_times),
            'queries_per_sec': 1.0 / (sum(query_times) / len(query_times))
        }

# â”€â”€ å¹¶å‘æµ‹è¯• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def apex_worker(thread_id: int, db_path: str, ops_count: int) -> Dict:
    """ApexBaseå·¥ä½œçº¿ç¨‹"""
    client = ApexClient(db_path)
    results = {
        'thread_id': thread_id,
        'operations': 0,
        'errors': 0,
        'read_times': [],
        'write_times': [],
        'total_time': 0
    }
    
    start_time = time.perf_counter()
    
    try:
        client.use_table('sales_data')
        
        for i in range(ops_count):
            op_start = time.perf_counter()
            
            if random.random() < 0.8:  # 80%è¯»æ“ä½œ
                if random.random() < 0.6:
                    result = client.execute(f"SELECT * FROM sales_data WHERE _id = {random.randint(1, 100000)}")
                else:
                    result = client.execute("SELECT COUNT(*) FROM sales_data WHERE city = 'City_1'")
                
                results['read_times'].append(time.perf_counter() - op_start)
            else:  # 20%å†™æ“ä½œ
                new_record = {
                    'user_id': random.randint(1, 50000),
                    'category': f"Category_{random.randint(1, 20)}",
                    'city': f"City_{random.randint(1, 10)}",
                    'price': random.uniform(10.0, 1000.0),
                    'quantity': random.randint(1, 100),
                    'timestamp': int(time.time()),
                    'is_active': random.choice([True, False]),
                    'score': random.uniform(0.0, 1.0)
                }
                
                client.store(new_record)
                results['write_times'].append(time.perf_counter() - op_start)
            
            results['operations'] += 1
            
    except Exception as e:
        results['errors'] += 1
    finally:
        try:
            client.close()
        except:
            pass
        results['total_time'] = time.perf_counter() - start_time
    
    return results

def sqlite_worker(thread_id: int, db_path: str, ops_count: int) -> Dict:
    """SQLiteå·¥ä½œçº¿ç¨‹"""
    conn = sqlite3.connect(db_path, timeout=30.0)
    results = {
        'thread_id': thread_id,
        'operations': 0,
        'errors': 0,
        'read_times': [],
        'write_times': [],
        'total_time': 0
    }
    
    start_time = time.perf_counter()
    
    try:
        for i in range(ops_count):
            op_start = time.perf_counter()
            
            if random.random() < 0.8:  # 80%è¯»æ“ä½œ
                if random.random() < 0.6:
                    result = conn.execute(f"SELECT * FROM sales_data WHERE _id = {random.randint(1, 100000)}").fetchall()
                else:
                    result = conn.execute("SELECT COUNT(*) FROM sales_data WHERE city = 'City_1'").fetchall()
                
                results['read_times'].append(time.perf_counter() - op_start)
            else:  # 20%å†™æ“ä½œ
                new_record = (
                    random.randint(1, 50000),
                    f"Category_{random.randint(1, 20)}",
                    f"City_{random.randint(1, 10)}",
                    random.uniform(10.0, 1000.0),
                    random.randint(1, 100),
                    int(time.time()),
                    random.choice([True, False]),
                    random.uniform(0.0, 1.0)
                )
                
                conn.execute('''
                    INSERT INTO sales_data (user_id, category, city, price, quantity, timestamp, is_active, score)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', new_record)
                conn.commit()
                results['write_times'].append(time.perf_counter() - op_start)
            
            results['operations'] += 1
            
    except Exception as e:
        results['errors'] += 1
    finally:
        conn.close()
        results['total_time'] = time.perf_counter() - start_time
    
    return results

def duckdb_worker(thread_id: int, db_path: str, ops_count: int) -> Dict:
    """DuckDBå·¥ä½œçº¿ç¨‹"""
    conn = duckdb.connect(db_path)
    results = {
        'thread_id': thread_id,
        'operations': 0,
        'errors': 0,
        'read_times': [],
        'write_times': [],
        'total_time': 0
    }
    
    start_time = time.perf_counter()
    
    try:
        for i in range(ops_count):
            op_start = time.perf_counter()
            
            if random.random() < 0.8:  # 80%è¯»æ“ä½œ
                if random.random() < 0.6:
                    result = conn.execute(f"SELECT * FROM sales_data WHERE _id = {random.randint(1, 100000)}").fetchall()
                else:
                    result = conn.execute("SELECT COUNT(*) FROM sales_data WHERE city = 'City_1'").fetchall()
                
                results['read_times'].append(time.perf_counter() - op_start)
            else:  # 20%å†™æ“ä½œ
                new_record = {
                    'user_id': random.randint(1, 50000),
                    'category': f"Category_{random.randint(1, 20)}",
                    'city': f"City_{random.randint(1, 10)}",
                    'price': random.uniform(10.0, 1000.0),
                    'quantity': random.randint(1, 100),
                    'timestamp': int(time.time()),
                    'is_active': random.choice([True, False]),
                    'score': random.uniform(0.0, 1.0)
                }
                
                conn.execute('''
                    INSERT INTO sales_data VALUES 
                    (NULL, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', [
                    new_record['user_id'], new_record['category'], new_record['city'],
                    new_record['price'], new_record['quantity'], new_record['timestamp'],
                    new_record['is_active'], new_record['score']
                ])
                results['write_times'].append(time.perf_counter() - op_start)
            
            results['operations'] += 1
            
    except Exception as e:
        results['errors'] += 1
    finally:
        conn.close()
        results['total_time'] = time.perf_counter() - start_time
    
    return results

@timer
def test_concurrent(db_path: str, worker_func, db_name: str) -> Dict:
    """å¹¶å‘æµ‹è¯•"""
    print(f"    å¯åŠ¨ {config.online_concurrent_threads} ä¸ª {db_name} å¹¶å‘çº¿ç¨‹...")
    
    with ThreadPoolExecutor(max_workers=config.online_concurrent_threads) as executor:
        futures = []
        for i in range(config.online_concurrent_threads):
            future = executor.submit(worker_func, i, db_path, config.online_ops_per_thread)
            futures.append(future)
        
        all_results = []
        for future in as_completed(futures):
            result = future.result()
            all_results.append(result)
    
    total_ops = sum(r['operations'] for r in all_results)
    total_errors = sum(r['errors'] for r in all_results)
    all_read_times = []
    all_write_times = []
    
    for r in all_results:
        all_read_times.extend(r['read_times'])
        all_write_times.extend(r['write_times'])
    
    return {
        'db_name': db_name,
        'threads': config.online_concurrent_threads,
        'total_operations': total_ops,
        'total_errors': total_errors,
        'error_rate': total_errors / total_ops if total_ops > 0 else 0,
        'ops_per_sec': total_ops / sum(r['total_time'] for r in all_results),
        'avg_read_time': sum(all_read_times) / len(all_read_times) if all_read_times else 0,
        'avg_write_time': sum(all_write_times) / len(all_write_times) if all_write_times else 0,
        'p95_read_time': np.percentile(all_read_times, 95) if all_read_times else 0,
        'p95_write_time': np.percentile(all_write_times, 95) if all_write_times else 0
    }

# â”€â”€ ä¸»æµ‹è¯•æµç¨‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    print("\n" + "="*80)
    print("  ä¸‰æ•°æ®åº“å‹åŠ›æµ‹è¯•å¯¹æ¯”: ApexBase vs SQLite vs DuckDB")
    print("="*80 + "\n")
    
    tmpdir = tempfile.mkdtemp(prefix="db_comparison_")
    
    db_paths = {
        'ApexBase': os.path.join(tmpdir, "apexbase.apex"),
        'SQLite': os.path.join(tmpdir, "sqlite.db"),
        'DuckDB': os.path.join(tmpdir, "duckdb.duckdb")
    }
    
    results = {}
    
    try:
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        print("ç”Ÿæˆæµ‹è¯•æ•°æ®...")
        data = generate_test_data(config.offline_batch_size)
        print(f"  æ•°æ®é‡: {len(data):,} æ¡è®°å½•\n")
        
        # â”€â”€ 1. ç¦»çº¿æ‰¹é‡å†™å…¥æµ‹è¯• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print("â”€" * 60)
        print("1. ç¦»çº¿æ‰¹é‡å†™å…¥æµ‹è¯•")
        print("â”€" * 60)
        
        # ApexBase
        if HAS_APEX:
            apex_tester = ApexBaseTester(db_paths['ApexBase'])
            apex_tester.setup()  # åˆ›å»ºè¡¨
            write_result, write_time = apex_tester.test_batch_write(data)
            results['ApexBase'] = {'write': write_result}
            # ä¸æ¸…ç†clientï¼Œä¿ç•™æ•°æ®ç”¨äºæŸ¥è¯¢æµ‹è¯•
            print(f"  ApexBase å†™å…¥: {write_result['rows_per_sec']:,.0f} rows/sec")
        
        # SQLite
        sqlite_tester = SQLiteTester(db_paths['SQLite'])
        sqlite_tester.setup()
        write_result, write_time = sqlite_tester.test_batch_write(data)
        results['SQLite'] = {'write': write_result}
        # ä¸å…³é—­è¿æ¥ï¼Œä¿ç•™æ•°æ®ç”¨äºæŸ¥è¯¢æµ‹è¯•
        print(f"  SQLite  å†™å…¥: {write_result['rows_per_sec']:,.0f} rows/sec")
        
        # DuckDB
        duckdb_tester = DuckDBTester(db_paths['DuckDB'])
        duckdb_tester.setup()
        write_result, write_time = duckdb_tester.test_batch_write(data)
        results['DuckDB'] = {'write': write_result}
        # ä¸å…³é—­è¿æ¥ï¼Œä¿ç•™æ•°æ®ç”¨äºæŸ¥è¯¢æµ‹è¯•
        print(f"  DuckDB   å†™å…¥: {write_result['rows_per_sec']:,.0f} rows/sec")
        
        # â”€â”€ 2. ç¦»çº¿æŸ¥è¯¢æµ‹è¯• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print("\nâ”€" * 60)
        print("2. ç¦»çº¿æŸ¥è¯¢æµ‹è¯•")
        print("â”€" * 60)
        
        # ApexBase - ä½¿ç”¨å·²æœ‰çš„client
        if HAS_APEX:
            query_result, query_time = apex_tester.test_queries()
            results['ApexBase']['query'] = query_result
            apex_tester.cleanup()
            print(f"  ApexBase æŸ¥è¯¢: {query_result['queries_per_sec']:.1f} queries/sec")
        
        # SQLite - ä½¿ç”¨å·²æœ‰çš„è¿æ¥
        query_result, query_time = sqlite_tester.test_queries()
        results['SQLite']['query'] = query_result
        sqlite_tester.cleanup()
        print(f"  SQLite  æŸ¥è¯¢: {query_result['queries_per_sec']:.1f} queries/sec")
        
        # DuckDB - ä½¿ç”¨å·²æœ‰çš„è¿æ¥
        query_result, query_time = duckdb_tester.test_queries()
        results['DuckDB']['query'] = query_result
        duckdb_tester.cleanup()
        print(f"  DuckDB   æŸ¥è¯¢: {query_result['queries_per_sec']:.1f} queries/sec")
        
        # â”€â”€ 3. åœ¨çº¿å¹¶å‘æµ‹è¯• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print("\nâ”€" * 60)
        print("3. åœ¨çº¿å¹¶å‘æµ‹è¯•")
        print("â”€" * 60)
        
        # ApexBase - ç°åœ¨æ”¯æŒå¤šå®¢æˆ·ç«¯è¿æ¥
        if HAS_APEX:
            concurrent_result, concurrent_time = test_concurrent(
                db_paths['ApexBase'], apex_worker, 'ApexBase'
            )
            results['ApexBase']['concurrent'] = concurrent_result
            print(f"  ApexBase å¹¶å‘: {concurrent_result['ops_per_sec']:.0f} ops/sec, é”™è¯¯ç‡: {concurrent_result['error_rate']*100:.1f}%")
        
        # SQLite
        concurrent_result, concurrent_time = test_concurrent(
            db_paths['SQLite'], sqlite_worker, 'SQLite'
        )
        results['SQLite']['concurrent'] = concurrent_result
        print(f"  SQLite  å¹¶å‘: {concurrent_result['ops_per_sec']:.0f} ops/sec, é”™è¯¯ç‡: {concurrent_result['error_rate']*100:.1f}%")
        
        # DuckDB
        concurrent_result, concurrent_time = test_concurrent(
            db_paths['DuckDB'], duckdb_worker, 'DuckDB'
        )
        results['DuckDB']['concurrent'] = concurrent_result
        print(f"  DuckDB   å¹¶å‘: {concurrent_result['ops_per_sec']:.0f} ops/sec, é”™è¯¯ç‡: {concurrent_result['error_rate']*100:.1f}%")
        
        # â”€â”€ 4. è¯¦ç»†å¯¹æ¯”åˆ†æ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print("\nâ”€" * 60)
        print("4. è¯¦ç»†å¯¹æ¯”åˆ†æ")
        print("â”€" * 60)
        
        print(f"\n{'æ•°æ®åº“':<10} {'å†™å…¥(rows/sec)':<15} {'æŸ¥è¯¢(q/sec)':<12} {'å¹¶å‘(ops/sec)':<15} {'é”™è¯¯ç‡(%)':<10} {'è¯»å»¶è¿Ÿ(ms)':<12} {'å†™å»¶è¿Ÿ(ms)':<12}")
        print("-" * 90)
        
        for db_name in ['ApexBase', 'SQLite', 'DuckDB']:
            if db_name not in results:
                continue
                
            r = results[db_name]
            write_speed = r['write']['rows_per_sec'] if 'write' in r else 0
            query_speed = r['query']['queries_per_sec'] if 'query' in r else 0
            concurrent_speed = r['concurrent']['ops_per_sec'] if 'concurrent' in r else 0
            error_rate = r['concurrent']['error_rate'] * 100 if 'concurrent' in r else 0
            read_latency = r['concurrent']['avg_read_time'] * 1000 if 'concurrent' in r else 0
            write_latency = r['concurrent']['avg_write_time'] * 1000 if 'concurrent' in r else 0
            
            print(f"{db_name:<10} {write_speed:<15,.0f} {query_speed:<12.1f} {concurrent_speed:<15,.0f} {error_rate:<10.1f} {read_latency:<12.1f} {write_latency:<12.1f}")
        
        # â”€â”€ 5. æ€§èƒ½æ’å â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print(f"\nâ”€" * 60)
        print("5. æ€§èƒ½æ’å")
        print("â”€" * 60)
        
        rankings = {
            'å†™å…¥æ€§èƒ½': {},
            'æŸ¥è¯¢æ€§èƒ½': {},
            'å¹¶å‘æ€§èƒ½': {},
            'ç¨³å®šæ€§': {}
        }
        
        for db_name in ['ApexBase', 'SQLite', 'DuckDB']:
            if db_name not in results:
                continue
                
            r = results[db_name]
            rankings['å†™å…¥æ€§èƒ½'][db_name] = r['write']['rows_per_sec'] if 'write' in r else 0
            rankings['æŸ¥è¯¢æ€§èƒ½'][db_name] = r['query']['queries_per_sec'] if 'query' in r else 0
            rankings['å¹¶å‘æ€§èƒ½'][db_name] = r['concurrent']['ops_per_sec'] if 'concurrent' in r else 0
            rankings['ç¨³å®šæ€§'][db_name] = 100 - (r['concurrent']['error_rate'] * 100) if 'concurrent' in r else 0
        
        for metric, scores in rankings.items():
            sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
            print(f"\n{metric}:")
            for i, (db, score) in enumerate(sorted_scores, 1):
                if metric == 'å†™å…¥æ€§èƒ½':
                    print(f"  {i}. {db}: {score:,.0f} rows/sec")
                elif metric == 'æŸ¥è¯¢æ€§èƒ½':
                    print(f"  {i}. {db}: {score:.1f} queries/sec")
                elif metric == 'å¹¶å‘æ€§èƒ½':
                    print(f"  {i}. {db}: {score:,.0f} ops/sec")
                else:  # ç¨³å®šæ€§
                    print(f"  {i}. {db}: {score:.1f}%")
        
        # â”€â”€ 6. æ€»ç»“å»ºè®® â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print(f"\nâ”€" * 60)
        print("6. æ€»ç»“å»ºè®®")
        print("â”€" * 60)
        
        print("\nğŸ¯ é€‚ç”¨åœºæ™¯æ¨è:")
        
        # å†™å…¥æ€§èƒ½æœ€ä½³
        best_write = max(rankings['å†™å…¥æ€§èƒ½'].items(), key=lambda x: x[1])
        print(f"ğŸ“ æ‰¹é‡å†™å…¥åœºæ™¯: {best_write[0]} (ä¼˜åŠ¿: {best_write[1]:,.0f} rows/sec)")
        
        # æŸ¥è¯¢æ€§èƒ½æœ€ä½³
        best_query = max(rankings['æŸ¥è¯¢æ€§èƒ½'].items(), key=lambda x: x[1])
        print(f"ğŸ” åˆ†ææŸ¥è¯¢åœºæ™¯: {best_query[0]} (ä¼˜åŠ¿: {best_query[1]:.1f} queries/sec)")
        
        # å¹¶å‘æ€§èƒ½æœ€ä½³
        best_concurrent = max(rankings['å¹¶å‘æ€§èƒ½'].items(), key=lambda x: x[1])
        print(f"âš¡ é«˜å¹¶å‘åœºæ™¯: {best_concurrent[0]} (ä¼˜åŠ¿: {best_concurrent[1]:,.0f} ops/sec)")
        
        # ç¨³å®šæ€§æœ€ä½³
        best_stability = max(rankings['ç¨³å®šæ€§'].items(), key=lambda x: x[1])
        print(f"ğŸ›¡ï¸  ç¨³å®šæ€§è¦æ±‚: {best_stability[0]} (ä¼˜åŠ¿: {best_stability[1]:.1f}% æ— é”™è¯¯)")
        
        print(f"\nğŸ’¡ ç»¼åˆè¯„ä¼°:")
        if HAS_APEX:
            apex_write = rankings['å†™å…¥æ€§èƒ½']['ApexBase']
            apex_query = rankings['æŸ¥è¯¢æ€§èƒ½']['ApexBase']
            apex_concurrent = rankings['å¹¶å‘æ€§èƒ½']['ApexBase']
            
            if apex_write > rankings['å†™å…¥æ€§èƒ½']['SQLite'] * 0.8:
                print("âœ… ApexBase åœ¨å†™å…¥åœºæ™¯è¡¨ç°ä¼˜å¼‚")
            if apex_query > rankings['æŸ¥è¯¢æ€§èƒ½']['SQLite'] * 0.5:
                print("âœ… ApexBase æŸ¥è¯¢æ€§èƒ½å¯æ¥å—")
            if apex_concurrent > rankings['å¹¶å‘æ€§èƒ½']['SQLite'] * 0.5:
                print("âœ… ApexBase å¹¶å‘å¤„ç†èƒ½åŠ›è‰¯å¥½")
        
        print(f"\nğŸ“Š æµ‹è¯•é…ç½®:")
        print(f"  â€¢ æ•°æ®è§„æ¨¡: {config.offline_batch_size:,} è¡Œ")
        print(f"  â€¢ å¹¶å‘çº¿ç¨‹: {config.online_concurrent_threads}")
        print(f"  â€¢ æ¯çº¿ç¨‹æ“ä½œ: {config.online_ops_per_thread}")
        print(f"  â€¢ æŸ¥è¯¢è¿­ä»£: {config.offline_query_iters}")
        
    finally:
        import shutil
        shutil.rmtree(tmpdir, ignore_errors=True)

if __name__ == "__main__":
    main()
